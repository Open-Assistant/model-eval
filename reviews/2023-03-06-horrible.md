# Model Overview

Reviewed using Open Assistant's [Model Output Comparer](https://open-assistant.github.io/oasst-model-eval/)

## Models

https://raw.githubusercontent.com/Open-Assistant/oasst-model-eval/main/sampling_reports/chip2_7b_instruct_alpha/2023-03-04_Rallio67_chip2_7B_instruct_alpha_sampling_synth2.json
https://raw.githubusercontent.com/Open-Assistant/oasst-model-eval/main/sampling_reports/pythia/2023-03-04_theblackcat102_pythia-1b-deduped-sft_sampling_noprefix.json
https://raw.githubusercontent.com/Open-Assistant/oasst-model-eval/main/sampling_reports/pythia/2023-03-04_theblackcat102_pythia-3b-deduped-sft_sampling_noprefix.json
https://raw.githubusercontent.com/Open-Assistant/oasst-model-eval/main/sampling_reports/pythia/2023-03-05_theblackcat102_pythia-12b-deduped-sft_sampling_noprefix.json
https://raw.githubusercontent.com/Open-Assistant/oasst-model-eval/main/sampling_reports/chip2_7b_instruct_alpha/2023-03-05_Rallio67_chip2_7B_instruct_alpha_sampling_synth2_lottery.json
https://raw.githubusercontent.com/Open-Assistant/oasst-model-eval/main/sampling_reports/pythia/2023-03-05_theblackcat102_pythia-3b-deduped-sft_sampling_noprefix_lottery.json

## Definitions

- **Accuracy (Ac):** How accurate the information given is.
- **Comprehension (Co):** How well the model understands the task given.
- **Personality (Pe):** How personal the replies feel, as opposed to neutral and objective statements.
- **Politeness (Po):** How polite the reply is, and whether it avoids rude or offensive language.
- **Length (Le):** Whether a reply tends to be too long or too short. 3 is ideal.
- **Scrubbed data (Sc):** Whether the reply contains information that is or appears to be scrubbed, such as PII or URLs.

## Ratings

| Name																 | Ac | Co | Pe | Po | Le | Sc | Notes |
|--------------------------------------------------------------------|----|----|----|----|----|----|-------|
| 2023-03-04_theblackcat102_pythia-3b-deduped-sft_sampling_noprefix  | 2  | 2  | 5  | 3  | 5  | 4  | Incorrect self-perception ("[...]here's what we've come up with so far[...]", "In Japan, where I am based currently, my current employer[...]" |
| 2023-03-05_theblackcat102_pythia-12b-deduped-sft_sampling_noprefix | 3  | 3  | 4  | 4  | 4  | 4  | Still same problem as above, only slightly less pronounced |
| 2023-03-04_Rallio67_chip2_7B_instruct_alpha_sampling_synth2        | 3  | 2  | 2  | 5  | 2  | 1  | Excessive use of list syntax |

## Notes

- "greedy" config is almost always bad, followed by "contrastive"
- There seemed to be no consistently best option, it tended to change from prompt to prompt
- Pythia feels like it is copying text from the internet, there were a lot of cases where it referred to itself as a particular person (someone living in New York, a user of a particular subreddit)
- Chip2 has a strong tendency to start with "1. ", even if it isn't required or helpful
